{
   "model_type": "roberta",
   "model_name": "roberta-base",
   "output_dir": "outputs/",
   "cache_dir": "cache/",
   "fp16": true,
   "fp16_opt_level": "O1",
   "max_seq_length": 128,
   "train_batch_size": 8,
   "eval_batch_size": 8,
   "gradient_accumulation_steps": 1,
   "num_train_epochs": 5,
   "weight_decay": 0,
   "learning_rate": 4e-5,
   "adam_epsilon": 1e-8,
   "warmup_ratio": 0.06,
   "warmup_steps": 0,
   "max_grad_norm": 1.0,
   "logging_steps": 50,
   "evaluate_during_training": false,
   "save_steps": 2000,
   "eval_all_checkpoints": true,
   "use_tensorboard": false,
   "overwrite_output_dir": true,
   "reprocess_input_data": false
}